import numpy as np

def compute_entropy(transition_matrix, letter_probabilities):
    """Compute the entropy of a first-order Markov source."""
    # Ensure the transition matrix is a numpy array
    transition_matrix = np.array(transition_matrix)
    
    # Get the states (letters)
    states = transition_matrix.index.tolist()  # Assuming transition_matrix is a DataFrame

    # Initialize entropy
    entropy = 0.0

    # Calculate entropy
    for i, state_i in enumerate(states):
        for j, state_j in enumerate(states):
            p_i = letter_probabilities.get(state_i, 0)  # Fixed probability of letter i
            p_ij = transition_matrix.iat[i, j]  # Transition probability from i to j
            
            if p_ij > 0:  # Only consider positive probabilities
                entropy += p_i * p_ij * np.log2(p_ij)  # Use log base 2

    return -entropy  # The entropy should be negative of the sum

# Example usage of compute_entropy function
# Assuming you have your transition matrix as transition_df and a dictionary of letter probabilities
letter_probabilities = {
    'e': 0.1611,
    'n': 0.1033,
    'i': 0.0905,
    'r': 0.0672,
    't': 0.0634,
    's': 0.0623,
    'a': 0.0560,
    'h': 0.0520,
    'd': 0.0417,
    'u': 0.0370,
    'c': 0.0340,
    'l': 0.0324,
    'g': 0.0294,
    'm': 0.0280,
    'o': 0.0232,
    'b': 0.0219,
    'f': 0.0171,
    'w': 0.0139,
    'z': 0.0136,
    'k': 0.0133,
    'v': 0.0092,
    'p': 0.0084,
    'ü': 0.0064,
    'ä': 0.0051,
    'ö': 0.0036,
    'ß': 0.0019,
    'j': 0.0019,
    'x': 0.0011,
    'q': 0.0007,
    'y': 0.0006
}

# Assuming transition_df is your transition DataFrame from the previous steps
entropy_value = compute_entropy(transition_df, letter_probabilities)
print(f"Entropy: {entropy_value:.4f} bits")
